{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a17f3d7c-a838-4e4e-9f78-7e2ad376ba10",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "file_path = \"~/messages_backup_24_2025.csv.gz\" # ../../messages_copy_12_22_25.csv.zst\" #\"../../messages.csv.gz\"\n",
    "#chunk_size = 500_000  # Adjust based on available RAM\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3712578c-16ac-4053-855a-3b9e37235eee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15f4befe-7017-4a18-8b4c-1f1448ada9ad",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "91ffc4a2-28d2-4bb0-8a7c-d14fbf7abb28",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data_iterator = pd.read_csv(file_path, chunksize=chunk_size)\n",
    "\n",
    "for idx, chunk in enumerate(data_iterator):\n",
    "    # Process your chunk (e.g., filtering, simple math)\n",
    "    #processed_chunk = chunk[chunk['value'] > 50]\n",
    "    #print(len(chunk))\n",
    "    #print(idx)\n",
    "    #if idx > 200:\n",
    "    #    break\n",
    "    # Save to a new file (append mode)\n",
    "    #processed_chunk.to_csv('output.csv', mode='a', index=False, header=not os.path.exists('output.csv'))\n",
    "\n",
    "    #print(chunk)\n",
    "    \n",
    "    if idx > 1:\n",
    "          break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c5489feb-f2f1-48e3-99e4-89c92155cf6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tqdm\n",
      "  Using cached tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n",
      "Using cached tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "Installing collected packages: tqdm\n",
      "Successfully installed tqdm-4.67.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ceedc774-2ca1-4c2d-83af-b3f475e65154",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.dataframe as dd\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "57867dcc-3438-4d70-bdf6-0a11378e68f4",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'os' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m output_dir = \u001b[33m\"\u001b[39m\u001b[33mmessages_parquet_zstd\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[43mos\u001b[49m.makedirs(output_dir, exist_ok=\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[31mNameError\u001b[39m: name 'os' is not defined"
     ]
    }
   ],
   "source": [
    "output_dir = \"messages_parquet_zstd\"\n",
    "os.makedirs(output_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "788add42-9292-4086-8f5d-c423e8d50cc1",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'massive_data.csv.gz'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 12\u001b[39m\n\u001b[32m      8\u001b[39m \u001b[38;5;66;03m# Process in 1-2GB chunks (uncompressed size)\u001b[39;00m\n\u001b[32m      9\u001b[39m \u001b[38;5;66;03m# For 3TB, aim for ~1000-2000 small parquet files\u001b[39;00m\n\u001b[32m     10\u001b[39m chunk_size = \u001b[32m1_000_000\u001b[39m \n\u001b[32m---> \u001b[39m\u001b[32m12\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i, chunk \u001b[38;5;129;01min\u001b[39;00m tqdm(\u001b[38;5;28menumerate\u001b[39m(\u001b[43mpd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mmassive_data.csv.gz\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchunksize\u001b[49m\u001b[43m=\u001b[49m\u001b[43mchunk_size\u001b[49m\u001b[43m)\u001b[49m)):\n\u001b[32m     13\u001b[39m     \u001b[38;5;66;03m# Convert to efficient dtypes (e.g., float32, category) to save space\u001b[39;00m\n\u001b[32m     14\u001b[39m     \u001b[38;5;66;03m# chunk['col'] = chunk['col'].astype('category') \u001b[39;00m\n\u001b[32m     16\u001b[39m     file_path = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00moutput_dir\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m/part_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m04d\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.parquet\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     17\u001b[39m     chunk.to_parquet(file_path, engine=\u001b[33m'\u001b[39m\u001b[33mpyarrow\u001b[39m\u001b[33m'\u001b[39m, compression=\u001b[33m'\u001b[39m\u001b[33mzstd\u001b[39m\u001b[33m'\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/science/lib/python3.14/site-packages/pandas/io/parsers/readers.py:1026\u001b[39m, in \u001b[36mread_csv\u001b[39m\u001b[34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[39m\n\u001b[32m   1013\u001b[39m kwds_defaults = _refine_defaults_read(\n\u001b[32m   1014\u001b[39m     dialect,\n\u001b[32m   1015\u001b[39m     delimiter,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1022\u001b[39m     dtype_backend=dtype_backend,\n\u001b[32m   1023\u001b[39m )\n\u001b[32m   1024\u001b[39m kwds.update(kwds_defaults)\n\u001b[32m-> \u001b[39m\u001b[32m1026\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/science/lib/python3.14/site-packages/pandas/io/parsers/readers.py:620\u001b[39m, in \u001b[36m_read\u001b[39m\u001b[34m(filepath_or_buffer, kwds)\u001b[39m\n\u001b[32m    617\u001b[39m _validate_names(kwds.get(\u001b[33m\"\u001b[39m\u001b[33mnames\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[32m    619\u001b[39m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m620\u001b[39m parser = \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    622\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[32m    623\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/science/lib/python3.14/site-packages/pandas/io/parsers/readers.py:1620\u001b[39m, in \u001b[36mTextFileReader.__init__\u001b[39m\u001b[34m(self, f, engine, **kwds)\u001b[39m\n\u001b[32m   1617\u001b[39m     \u001b[38;5;28mself\u001b[39m.options[\u001b[33m\"\u001b[39m\u001b[33mhas_index_names\u001b[39m\u001b[33m\"\u001b[39m] = kwds[\u001b[33m\"\u001b[39m\u001b[33mhas_index_names\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m   1619\u001b[39m \u001b[38;5;28mself\u001b[39m.handles: IOHandles | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1620\u001b[39m \u001b[38;5;28mself\u001b[39m._engine = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/science/lib/python3.14/site-packages/pandas/io/parsers/readers.py:1880\u001b[39m, in \u001b[36mTextFileReader._make_engine\u001b[39m\u001b[34m(self, f, engine)\u001b[39m\n\u001b[32m   1878\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[32m   1879\u001b[39m         mode += \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m1880\u001b[39m \u001b[38;5;28mself\u001b[39m.handles = \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1881\u001b[39m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1882\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1883\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mencoding\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1884\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcompression\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1885\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmemory_map\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1886\u001b[39m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[43m=\u001b[49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1887\u001b[39m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mencoding_errors\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstrict\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1888\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstorage_options\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1889\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1890\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m.handles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1891\u001b[39m f = \u001b[38;5;28mself\u001b[39m.handles.handle\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/science/lib/python3.14/site-packages/pandas/io/common.py:765\u001b[39m, in \u001b[36mget_handle\u001b[39m\u001b[34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[39m\n\u001b[32m    761\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m compression == \u001b[33m\"\u001b[39m\u001b[33mgzip\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    762\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[32m    763\u001b[39m         \u001b[38;5;66;03m# error: Incompatible types in assignment (expression has type\u001b[39;00m\n\u001b[32m    764\u001b[39m         \u001b[38;5;66;03m# \"GzipFile\", variable has type \"Union[str, BaseBuffer]\")\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m765\u001b[39m         handle = \u001b[43mgzip\u001b[49m\u001b[43m.\u001b[49m\u001b[43mGzipFile\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[assignment]\u001b[39;49;00m\n\u001b[32m    766\u001b[39m \u001b[43m            \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    767\u001b[39m \u001b[43m            \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m=\u001b[49m\u001b[43mioargs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    768\u001b[39m \u001b[43m            \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mcompression_args\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    769\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    770\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    771\u001b[39m         handle = gzip.GzipFile(\n\u001b[32m    772\u001b[39m             \u001b[38;5;66;03m# No overload variant of \"GzipFile\" matches argument types\u001b[39;00m\n\u001b[32m    773\u001b[39m             \u001b[38;5;66;03m# \"Union[str, BaseBuffer]\", \"str\", \"Dict[str, Any]\"\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    776\u001b[39m             **compression_args,\n\u001b[32m    777\u001b[39m         )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/science/lib/python3.14/gzip.py:208\u001b[39m, in \u001b[36mGzipFile.__init__\u001b[39m\u001b[34m(self, filename, mode, compresslevel, fileobj, mtime)\u001b[39m\n\u001b[32m    206\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    207\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m fileobj \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m208\u001b[39m         fileobj = \u001b[38;5;28mself\u001b[39m.myfileobj = \u001b[43mbuiltins\u001b[49m\u001b[43m.\u001b[49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mrb\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    209\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m filename \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    210\u001b[39m         filename = \u001b[38;5;28mgetattr\u001b[39m(fileobj, \u001b[33m'\u001b[39m\u001b[33mname\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33m'\u001b[39m)\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: 'massive_data.csv.gz'"
     ]
    }
   ],
   "source": [
    "# simple version just directly copies to parquet!!!\n",
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "output_dir = \"messages_parquet_zstd\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Process in 1-2GB chunks (uncompressed size)\n",
    "# For 3TB, aim for ~1000-2000 small parquet files\n",
    "chunk_size = 1_000_000 \n",
    "\n",
    "for i, chunk in tqdm(enumerate(pd.read_csv(file_path, chunksize=chunk_size))):\n",
    "    # Convert to efficient dtypes (e.g., float32, category) to save space\n",
    "    # chunk['col'] = chunk['col'].astype('category') \n",
    "    \n",
    "    file_path = f\"{output_dir}/part_{i:04d}.parquet\"\n",
    "    chunk.to_parquet(file_path, engine='pyarrow', compression='zstd')\n",
    "    print(f\"Saved chunk {i}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a2922b3-dc6b-41a0-b300-ca72c2208cba",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    }
   ],
   "source": [
    "# fancy version converts from json/python data types to ones that can be loaded more efficiently!!!\n",
    "import os\n",
    "import pandas as pd\n",
    "import json\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "from tqdm import tqdm\n",
    "\n",
    "file_path = \"/media/pwhiddy/DATA/linux_dat/poke_map_viz_logs/messages.csv.gz\" #\"/home/pwhiddy/messages_backup_24_2025.csv.gz\"\n",
    "output_dir = \"/home/pwhiddy/messages_1_15_2026_parquet_zstd_v2_env_id/data\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# 1. Initialize the CSV reader (iterator)\n",
    "reader = pd.read_csv(file_path, chunksize=400_000) # Smaller chunks for JSON overhead\n",
    "error_log = \"processing_errors.log\"\n",
    "\n",
    "def process_chunk_optimized(chunk):\n",
    "    def parse(msg):\n",
    "        try:\n",
    "            d = json.loads(msg)\n",
    "            m = d.get('metadata', {})\n",
    "            \n",
    "            # Extract and sanitize\n",
    "            user = str(m.get('user', '')).strip()\n",
    "            env_id = str(m.get('env_id', '')).strip()\n",
    "            color = str(m.get('color', '')).strip()\n",
    "            extra = str(m.get('extra', '')).strip()\n",
    "            \n",
    "            # Convert empty strings to None for Arrow/Parquet safety\n",
    "            return (\n",
    "                user if user != '' else None,\n",
    "                env_id if env_id != '' else None,\n",
    "                color if color != '' else None,\n",
    "                extra if extra != '' else None,\n",
    "                d.get('coords', [])\n",
    "            )\n",
    "        except Exception as e:\n",
    "            with open(error_log, \"a\") as f:\n",
    "                f.write(f\"Error in chunk {i}: {str(e)}\\n\")\n",
    "            return (None, None, None, [])\n",
    "\n",
    "    parsed = [parse(m) for m in chunk['message']]\n",
    "    users, env_ids, colors, extras, coords = zip(*parsed)\n",
    "\n",
    "    # BUILD THE DATAFRAME\n",
    "    new_df = pd.DataFrame({\n",
    "        # .reset_index(drop=True) is the key fix here!\n",
    "        'timestamp': pd.to_datetime(chunk['timestamp'], format='ISO8601').reset_index(drop=True),\n",
    "        'user': pd.Series(users, dtype='category'),\n",
    "        'env_id': pd.Series(env_ids, dtype='category'),\n",
    "        'color': pd.Series(colors, dtype='category'),\n",
    "        'extra': pd.Series(extras),\n",
    "        'coords': coords \n",
    "    })\n",
    "    return new_df\n",
    "\n",
    "for i, chunk in tqdm(enumerate(reader)):\n",
    "\n",
    "    try:\n",
    "        final_chunk = process_chunk_optimized(chunk)\n",
    "        \n",
    "        # 3. Write with PyArrow optimizations\n",
    "        final_chunk.to_parquet(\n",
    "            f'{output_dir}/part_{i:06d}.parquet', \n",
    "            engine='pyarrow', \n",
    "            compression='zstd',\n",
    "            compression_level=17, \n",
    "            # Utilize all cores for column-wise compression\n",
    "            #use_threads=True,\n",
    "            index=False\n",
    "        )\n",
    "\n",
    "    except Exception as e:\n",
    "        with open(error_log, \"a\") as f:\n",
    "            f.write(f\"Error in chunk {i}: {str(e)}\\n\")\n",
    "        continue # Move to the next chunk instead of crashing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31db4bdc-a18b-4982-95f3-0c79a095f6ac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aed5711a-c59a-41f3-8b88-d81eb1ad709a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be13c51f-bfd9-44e4-9902-af3e3d869a55",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.14",
   "language": "python",
   "name": "python314"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
