{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a17f3d7c-a838-4e4e-9f78-7e2ad376ba10",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "file_path = \"~/messages_backup_24_2025.csv.gz\" # ../../messages_copy_12_22_25.csv.zst\" #\"../../messages.csv.gz\"\n",
    "#chunk_size = 500_000  # Adjust based on available RAM\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3712578c-16ac-4053-855a-3b9e37235eee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15f4befe-7017-4a18-8b4c-1f1448ada9ad",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "91ffc4a2-28d2-4bb0-8a7c-d14fbf7abb28",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data_iterator = pd.read_csv(file_path, chunksize=chunk_size)\n",
    "\n",
    "for idx, chunk in enumerate(data_iterator):\n",
    "    # Process your chunk (e.g., filtering, simple math)\n",
    "    #processed_chunk = chunk[chunk['value'] > 50]\n",
    "    #print(len(chunk))\n",
    "    #print(idx)\n",
    "    #if idx > 200:\n",
    "    #    break\n",
    "    # Save to a new file (append mode)\n",
    "    #processed_chunk.to_csv('output.csv', mode='a', index=False, header=not os.path.exists('output.csv'))\n",
    "\n",
    "    #print(chunk)\n",
    "    \n",
    "    if idx > 1:\n",
    "          break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c5489feb-f2f1-48e3-99e4-89c92155cf6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tqdm\n",
      "  Using cached tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n",
      "Using cached tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "Installing collected packages: tqdm\n",
      "Successfully installed tqdm-4.67.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ceedc774-2ca1-4c2d-83af-b3f475e65154",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.dataframe as dd\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "57867dcc-3438-4d70-bdf6-0a11378e68f4",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'os' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m output_dir = \u001b[33m\"\u001b[39m\u001b[33mmessages_parquet_zstd\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[43mos\u001b[49m.makedirs(output_dir, exist_ok=\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[31mNameError\u001b[39m: name 'os' is not defined"
     ]
    }
   ],
   "source": [
    "output_dir = \"messages_parquet_zstd\"\n",
    "os.makedirs(output_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "788add42-9292-4086-8f5d-c423e8d50cc1",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'massive_data.csv.gz'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 12\u001b[39m\n\u001b[32m      8\u001b[39m \u001b[38;5;66;03m# Process in 1-2GB chunks (uncompressed size)\u001b[39;00m\n\u001b[32m      9\u001b[39m \u001b[38;5;66;03m# For 3TB, aim for ~1000-2000 small parquet files\u001b[39;00m\n\u001b[32m     10\u001b[39m chunk_size = \u001b[32m1_000_000\u001b[39m \n\u001b[32m---> \u001b[39m\u001b[32m12\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i, chunk \u001b[38;5;129;01min\u001b[39;00m tqdm(\u001b[38;5;28menumerate\u001b[39m(\u001b[43mpd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mmassive_data.csv.gz\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchunksize\u001b[49m\u001b[43m=\u001b[49m\u001b[43mchunk_size\u001b[49m\u001b[43m)\u001b[49m)):\n\u001b[32m     13\u001b[39m     \u001b[38;5;66;03m# Convert to efficient dtypes (e.g., float32, category) to save space\u001b[39;00m\n\u001b[32m     14\u001b[39m     \u001b[38;5;66;03m# chunk['col'] = chunk['col'].astype('category') \u001b[39;00m\n\u001b[32m     16\u001b[39m     file_path = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00moutput_dir\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m/part_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m04d\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.parquet\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     17\u001b[39m     chunk.to_parquet(file_path, engine=\u001b[33m'\u001b[39m\u001b[33mpyarrow\u001b[39m\u001b[33m'\u001b[39m, compression=\u001b[33m'\u001b[39m\u001b[33mzstd\u001b[39m\u001b[33m'\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/science/lib/python3.14/site-packages/pandas/io/parsers/readers.py:1026\u001b[39m, in \u001b[36mread_csv\u001b[39m\u001b[34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[39m\n\u001b[32m   1013\u001b[39m kwds_defaults = _refine_defaults_read(\n\u001b[32m   1014\u001b[39m     dialect,\n\u001b[32m   1015\u001b[39m     delimiter,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1022\u001b[39m     dtype_backend=dtype_backend,\n\u001b[32m   1023\u001b[39m )\n\u001b[32m   1024\u001b[39m kwds.update(kwds_defaults)\n\u001b[32m-> \u001b[39m\u001b[32m1026\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/science/lib/python3.14/site-packages/pandas/io/parsers/readers.py:620\u001b[39m, in \u001b[36m_read\u001b[39m\u001b[34m(filepath_or_buffer, kwds)\u001b[39m\n\u001b[32m    617\u001b[39m _validate_names(kwds.get(\u001b[33m\"\u001b[39m\u001b[33mnames\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[32m    619\u001b[39m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m620\u001b[39m parser = \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    622\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[32m    623\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/science/lib/python3.14/site-packages/pandas/io/parsers/readers.py:1620\u001b[39m, in \u001b[36mTextFileReader.__init__\u001b[39m\u001b[34m(self, f, engine, **kwds)\u001b[39m\n\u001b[32m   1617\u001b[39m     \u001b[38;5;28mself\u001b[39m.options[\u001b[33m\"\u001b[39m\u001b[33mhas_index_names\u001b[39m\u001b[33m\"\u001b[39m] = kwds[\u001b[33m\"\u001b[39m\u001b[33mhas_index_names\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m   1619\u001b[39m \u001b[38;5;28mself\u001b[39m.handles: IOHandles | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1620\u001b[39m \u001b[38;5;28mself\u001b[39m._engine = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/science/lib/python3.14/site-packages/pandas/io/parsers/readers.py:1880\u001b[39m, in \u001b[36mTextFileReader._make_engine\u001b[39m\u001b[34m(self, f, engine)\u001b[39m\n\u001b[32m   1878\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[32m   1879\u001b[39m         mode += \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m1880\u001b[39m \u001b[38;5;28mself\u001b[39m.handles = \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1881\u001b[39m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1882\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1883\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mencoding\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1884\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcompression\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1885\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmemory_map\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1886\u001b[39m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[43m=\u001b[49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1887\u001b[39m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mencoding_errors\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstrict\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1888\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstorage_options\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1889\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1890\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m.handles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1891\u001b[39m f = \u001b[38;5;28mself\u001b[39m.handles.handle\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/science/lib/python3.14/site-packages/pandas/io/common.py:765\u001b[39m, in \u001b[36mget_handle\u001b[39m\u001b[34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[39m\n\u001b[32m    761\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m compression == \u001b[33m\"\u001b[39m\u001b[33mgzip\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    762\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[32m    763\u001b[39m         \u001b[38;5;66;03m# error: Incompatible types in assignment (expression has type\u001b[39;00m\n\u001b[32m    764\u001b[39m         \u001b[38;5;66;03m# \"GzipFile\", variable has type \"Union[str, BaseBuffer]\")\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m765\u001b[39m         handle = \u001b[43mgzip\u001b[49m\u001b[43m.\u001b[49m\u001b[43mGzipFile\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[assignment]\u001b[39;49;00m\n\u001b[32m    766\u001b[39m \u001b[43m            \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    767\u001b[39m \u001b[43m            \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m=\u001b[49m\u001b[43mioargs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    768\u001b[39m \u001b[43m            \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mcompression_args\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    769\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    770\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    771\u001b[39m         handle = gzip.GzipFile(\n\u001b[32m    772\u001b[39m             \u001b[38;5;66;03m# No overload variant of \"GzipFile\" matches argument types\u001b[39;00m\n\u001b[32m    773\u001b[39m             \u001b[38;5;66;03m# \"Union[str, BaseBuffer]\", \"str\", \"Dict[str, Any]\"\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    776\u001b[39m             **compression_args,\n\u001b[32m    777\u001b[39m         )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/science/lib/python3.14/gzip.py:208\u001b[39m, in \u001b[36mGzipFile.__init__\u001b[39m\u001b[34m(self, filename, mode, compresslevel, fileobj, mtime)\u001b[39m\n\u001b[32m    206\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    207\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m fileobj \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m208\u001b[39m         fileobj = \u001b[38;5;28mself\u001b[39m.myfileobj = \u001b[43mbuiltins\u001b[49m\u001b[43m.\u001b[49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mrb\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    209\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m filename \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    210\u001b[39m         filename = \u001b[38;5;28mgetattr\u001b[39m(fileobj, \u001b[33m'\u001b[39m\u001b[33mname\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33m'\u001b[39m)\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: 'massive_data.csv.gz'"
     ]
    }
   ],
   "source": [
    "# simple version just directly copies to parquet!!!\n",
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "output_dir = \"messages_parquet_zstd\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Process in 1-2GB chunks (uncompressed size)\n",
    "# For 3TB, aim for ~1000-2000 small parquet files\n",
    "chunk_size = 1_000_000 \n",
    "\n",
    "for i, chunk in tqdm(enumerate(pd.read_csv(file_path, chunksize=chunk_size))):\n",
    "    # Convert to efficient dtypes (e.g., float32, category) to save space\n",
    "    # chunk['col'] = chunk['col'].astype('category') \n",
    "    \n",
    "    file_path = f\"{output_dir}/part_{i:04d}.parquet\"\n",
    "    chunk.to_parquet(file_path, engine='pyarrow', compression='zstd')\n",
    "    print(f\"Saved chunk {i}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4a2922b3-dc6b-41a0-b300-ca72c2208cba",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1854it [42:48:58, 83.14s/it] \n"
     ]
    },
    {
     "ename": "error",
     "evalue": "Error -3 while decompressing data: invalid code lengths set",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31merror\u001b[39m                                     Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 54\u001b[39m\n\u001b[32m     44\u001b[39m     new_df = pd.DataFrame({\n\u001b[32m     45\u001b[39m         \u001b[38;5;66;03m# .reset_index(drop=True) is the key fix here!\u001b[39;00m\n\u001b[32m     46\u001b[39m         \u001b[33m'\u001b[39m\u001b[33mtimestamp\u001b[39m\u001b[33m'\u001b[39m: pd.to_datetime(chunk[\u001b[33m'\u001b[39m\u001b[33mtimestamp\u001b[39m\u001b[33m'\u001b[39m], \u001b[38;5;28mformat\u001b[39m=\u001b[33m'\u001b[39m\u001b[33mISO8601\u001b[39m\u001b[33m'\u001b[39m).reset_index(drop=\u001b[38;5;28;01mTrue\u001b[39;00m),\n\u001b[32m   (...)\u001b[39m\u001b[32m     50\u001b[39m         \u001b[33m'\u001b[39m\u001b[33mcoords\u001b[39m\u001b[33m'\u001b[39m: coords \n\u001b[32m     51\u001b[39m     })\n\u001b[32m     52\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m new_df\n\u001b[32m---> \u001b[39m\u001b[32m54\u001b[39m \u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtqdm\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mreader\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m     56\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mtry\u001b[39;49;00m\u001b[43m:\u001b[49m\n\u001b[32m     57\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfinal_chunk\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mprocess_chunk_optimized\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/science/lib/python3.14/site-packages/tqdm/std.py:1181\u001b[39m, in \u001b[36mtqdm.__iter__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1178\u001b[39m time = \u001b[38;5;28mself\u001b[39m._time\n\u001b[32m   1180\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1181\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43miterable\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   1182\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01myield\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\n\u001b[32m   1183\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Update and possibly print the progressbar.\u001b[39;49;00m\n\u001b[32m   1184\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;49;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/science/lib/python3.14/site-packages/pandas/io/parsers/readers.py:1843\u001b[39m, in \u001b[36mTextFileReader.__next__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1841\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__next__\u001b[39m(\u001b[38;5;28mself\u001b[39m) -> DataFrame:\n\u001b[32m   1842\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1843\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mget_chunk\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1844\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[32m   1845\u001b[39m         \u001b[38;5;28mself\u001b[39m.close()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/science/lib/python3.14/site-packages/pandas/io/parsers/readers.py:1985\u001b[39m, in \u001b[36mTextFileReader.get_chunk\u001b[39m\u001b[34m(self, size)\u001b[39m\n\u001b[32m   1983\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m\n\u001b[32m   1984\u001b[39m     size = \u001b[38;5;28mmin\u001b[39m(size, \u001b[38;5;28mself\u001b[39m.nrows - \u001b[38;5;28mself\u001b[39m._currow)\n\u001b[32m-> \u001b[39m\u001b[32m1985\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnrows\u001b[49m\u001b[43m=\u001b[49m\u001b[43msize\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/science/lib/python3.14/site-packages/pandas/io/parsers/readers.py:1923\u001b[39m, in \u001b[36mTextFileReader.read\u001b[39m\u001b[34m(self, nrows)\u001b[39m\n\u001b[32m   1916\u001b[39m nrows = validate_integer(\u001b[33m\"\u001b[39m\u001b[33mnrows\u001b[39m\u001b[33m\"\u001b[39m, nrows)\n\u001b[32m   1917\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   1918\u001b[39m     \u001b[38;5;66;03m# error: \"ParserBase\" has no attribute \"read\"\u001b[39;00m\n\u001b[32m   1919\u001b[39m     (\n\u001b[32m   1920\u001b[39m         index,\n\u001b[32m   1921\u001b[39m         columns,\n\u001b[32m   1922\u001b[39m         col_dict,\n\u001b[32m-> \u001b[39m\u001b[32m1923\u001b[39m     ) = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[attr-defined]\u001b[39;49;00m\n\u001b[32m   1924\u001b[39m \u001b[43m        \u001b[49m\u001b[43mnrows\u001b[49m\n\u001b[32m   1925\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1926\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[32m   1927\u001b[39m     \u001b[38;5;28mself\u001b[39m.close()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/science/lib/python3.14/site-packages/pandas/io/parsers/c_parser_wrapper.py:234\u001b[39m, in \u001b[36mCParserWrapper.read\u001b[39m\u001b[34m(self, nrows)\u001b[39m\n\u001b[32m    232\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    233\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.low_memory:\n\u001b[32m--> \u001b[39m\u001b[32m234\u001b[39m         chunks = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_reader\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread_low_memory\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnrows\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    235\u001b[39m         \u001b[38;5;66;03m# destructive to chunks\u001b[39;00m\n\u001b[32m    236\u001b[39m         data = _concatenate_chunks(chunks)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/parsers.pyx:850\u001b[39m, in \u001b[36mpandas._libs.parsers.TextReader.read_low_memory\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/parsers.pyx:905\u001b[39m, in \u001b[36mpandas._libs.parsers.TextReader._read_rows\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/parsers.pyx:874\u001b[39m, in \u001b[36mpandas._libs.parsers.TextReader._tokenize_rows\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/parsers.pyx:891\u001b[39m, in \u001b[36mpandas._libs.parsers.TextReader._check_tokenize_status\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/parsers.pyx:2053\u001b[39m, in \u001b[36mpandas._libs.parsers.raise_parser_error\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/science/lib/python3.14/gzip.py:360\u001b[39m, in \u001b[36mGzipFile.read1\u001b[39m\u001b[34m(self, size)\u001b[39m\n\u001b[32m    358\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m size < \u001b[32m0\u001b[39m:\n\u001b[32m    359\u001b[39m     size = io.DEFAULT_BUFFER_SIZE\n\u001b[32m--> \u001b[39m\u001b[32m360\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_buffer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread1\u001b[49m\u001b[43m(\u001b[49m\u001b[43msize\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/science/lib/python3.14/compression/_common/_streams.py:68\u001b[39m, in \u001b[36mDecompressReader.readinto\u001b[39m\u001b[34m(self, b)\u001b[39m\n\u001b[32m     66\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mreadinto\u001b[39m(\u001b[38;5;28mself\u001b[39m, b):\n\u001b[32m     67\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mmemoryview\u001b[39m(b) \u001b[38;5;28;01mas\u001b[39;00m view, view.cast(\u001b[33m\"\u001b[39m\u001b[33mB\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m byte_view:\n\u001b[32m---> \u001b[39m\u001b[32m68\u001b[39m         data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mbyte_view\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     69\u001b[39m         byte_view[:\u001b[38;5;28mlen\u001b[39m(data)] = data\n\u001b[32m     70\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(data)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/science/lib/python3.14/gzip.py:580\u001b[39m, in \u001b[36m_GzipReader.read\u001b[39m\u001b[34m(self, size)\u001b[39m\n\u001b[32m    578\u001b[39m     uncompress = \u001b[38;5;28mself\u001b[39m._decompressor.decompress(buf, size)\n\u001b[32m    579\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m580\u001b[39m     uncompress = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_decompressor\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdecompress\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43mb\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msize\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    582\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._decompressor.unused_data != \u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    583\u001b[39m     \u001b[38;5;66;03m# Prepend the already read bytes to the fileobj so they can\u001b[39;00m\n\u001b[32m    584\u001b[39m     \u001b[38;5;66;03m# be seen by _read_eof() and _read_gzip_header()\u001b[39;00m\n\u001b[32m    585\u001b[39m     \u001b[38;5;28mself\u001b[39m._fp.prepend(\u001b[38;5;28mself\u001b[39m._decompressor.unused_data)\n",
      "\u001b[31merror\u001b[39m: Error -3 while decompressing data: invalid code lengths set"
     ]
    }
   ],
   "source": [
    "# fancy version converts from json/python data types to ones that can be loaded more efficiently!!!\n",
    "import os\n",
    "import pandas as pd\n",
    "import json\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "from tqdm import tqdm\n",
    "\n",
    "file_path = \"/home/pwhiddy/messages_backup_24_2025.csv.gz\"\n",
    "output_dir = \"/home/pwhiddy/messages_parquet_zstd\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# 1. Initialize the CSV reader (iterator)\n",
    "reader = pd.read_csv(file_path, chunksize=400_000) # Smaller chunks for JSON overhead\n",
    "error_log = \"processing_errors.log\"\n",
    "\n",
    "def process_chunk_optimized(chunk):\n",
    "    def parse(msg):\n",
    "        try:\n",
    "            d = json.loads(msg)\n",
    "            m = d.get('metadata', {})\n",
    "            \n",
    "            # Extract and sanitize\n",
    "            user = str(m.get('user', '')).strip()\n",
    "            color = str(m.get('color', '')).strip()\n",
    "            extra = str(m.get('extra', '')).strip()\n",
    "            \n",
    "            # Convert empty strings to None for Arrow/Parquet safety\n",
    "            return (\n",
    "                user if user != '' else None,\n",
    "                color if color != '' else None,\n",
    "                extra if extra != '' else None,\n",
    "                d.get('coords', [])\n",
    "            )\n",
    "        except Exception as e:\n",
    "            with open(error_log, \"a\") as f:\n",
    "                f.write(f\"Error in chunk {i}: {str(e)}\\n\")\n",
    "            return (None, None, None, [])\n",
    "\n",
    "    parsed = [parse(m) for m in chunk['message']]\n",
    "    users, colors, extras, coords = zip(*parsed)\n",
    "\n",
    "    # BUILD THE DATAFRAME\n",
    "    new_df = pd.DataFrame({\n",
    "        # .reset_index(drop=True) is the key fix here!\n",
    "        'timestamp': pd.to_datetime(chunk['timestamp'], format='ISO8601').reset_index(drop=True),\n",
    "        'user': pd.Series(users, dtype='category'),\n",
    "        'color': pd.Series(colors, dtype='category'),\n",
    "        'extra': pd.Series(extras),\n",
    "        'coords': coords \n",
    "    })\n",
    "    return new_df\n",
    "\n",
    "for i, chunk in tqdm(enumerate(reader)):\n",
    "\n",
    "    try:\n",
    "        final_chunk = process_chunk_optimized(chunk)\n",
    "        \n",
    "        # 3. Write with PyArrow optimizations\n",
    "        final_chunk.to_parquet(\n",
    "            f'{output_dir}/part_{i:06d}.parquet', \n",
    "            engine='pyarrow', \n",
    "            compression='zstd',\n",
    "            compression_level=17, \n",
    "            # Utilize all cores for column-wise compression\n",
    "            #use_threads=True,\n",
    "            index=False\n",
    "        )\n",
    "\n",
    "    except Exception as e:\n",
    "        with open(error_log, \"a\") as f:\n",
    "            f.write(f\"Error in chunk {i}: {str(e)}\\n\")\n",
    "        continue # Move to the next chunk instead of crashing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31db4bdc-a18b-4982-95f3-0c79a095f6ac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aed5711a-c59a-41f3-8b88-d81eb1ad709a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be13c51f-bfd9-44e4-9902-af3e3d869a55",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.14",
   "language": "python",
   "name": "python314"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
